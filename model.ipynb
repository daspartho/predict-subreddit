{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GikyAA73hoAy"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentencepiece datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, ClassLabel\n",
        "\n",
        "ds = load_dataset(\"daspartho/subreddit-posts\", split='train') # i've uploaded the dataset on HuggingFace Datasets :)\n",
        "\n",
        "label_names = ds.unique('label') # list of labels (subreddits)\n",
        "labels = ClassLabel(\n",
        "    num_classes=len(label_names), \n",
        "    names=label_names,\n",
        "    )\n",
        "\n",
        "ds = ds.train_test_split(test_size=0.2, shuffle=True)"
      ],
      "metadata": {
        "id": "Cl2fsohOh3s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tok_func(x): \n",
        "    tok_x = tokenizer(x[\"title\"], padding=True, truncation=True)\n",
        "    tok_x['label'] = labels.str2int(x['label'])\n",
        "    return tok_x\n",
        "\n",
        "tok_ds = ds.map(tok_func, batched=True)"
      ],
      "metadata": {
        "id": "o9rEG-aTiK0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "bs = 128\n",
        "epochs = 5\n",
        "lr = 5e-5\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "args = TrainingArguments('model', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True, \n",
        "                         evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2, \n",
        "                         num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_names))\n",
        "\n",
        "trainer = Trainer(model, args, train_dataset=tok_ds['train'], eval_dataset=tok_ds['test'], \n",
        "                  tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
        "\n",
        "trainer.train();"
      ],
      "metadata": {
        "id": "Bwdtu0GjnPfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "XoYciH7h_TIB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}